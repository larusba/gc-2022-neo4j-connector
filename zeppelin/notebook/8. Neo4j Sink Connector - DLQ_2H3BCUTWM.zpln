{
  "paragraphs": [
    {
      "text": "%md\nHere is a piece of JSON configuration we’re going to use to update an already existent connector, the one we had configured with the Cypher template strategy on the `movie` topic, and configuring also a DLQ topic.\n\n```\n{\n    \"topics\": \"movie\",\n    \"connector.class\": \"streams.kafka.connect.sink.Neo4jSinkConnector\",\n    \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"key.converter.schemas.enable\": false,\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter.schemas.enable\": false,\n    \"kafka.bootstrap.servers\": \"broker:9093\",\n    \"errors.tolerance\": \"all\",\n    \"errors.log.enable\": true,\n    \"errors.deadletterqueue.topic.name\": \"dlq-movie\",\n    \"errors.log.include.messages\": true,\n    \"errors.deadletterqueue.topic.replication.factor\": 1,\n    \"errors.deadletterqueue.context.headers.enable\": true,\n    \"neo4j.server.uri\": \"bolt://neo4j:7687\",\n    \"neo4j.authentication.basic.username\": \"neo4j\",\n    \"neo4j.authentication.basic.password\": \"password\",\n    \"neo4j.encryption.enabled\": false,\n    \"neo4j.topic.cypher.movie\": \"MERGE (m:Movie {title: event.title}) SET m.released\u003devent.released, m.tagline\u003devent.tagline, m.timestamp\u003dtimestamp() RETURN m\"\n}\n```\n\nTo create the connector instance we just need to submit the above JSON to the Kafka Connect REST endpoint, as follow:\n\n```\ncurl -X PUT http://localhost:8083/connectors/Neo4jSinkConnectorCypher/config -H \u0027Content-Type:application/json\u0027 -H \u0027Accept:application/json\u0027 -d @kafka-connect-sink-cypher-dlq.json\n```\n\nPlease note that we\u0027ve updated an already existing connector (in particular, the one we\u0027ve created for the cypher template strategy).\n\nNow our topics list should include also our new `dlq-movie` topic. We can verify it by running:\n\n```\ndocker exec -it broker kafka-topics --bootstrap-server broker:9092 --list\n```",
      "user": "anonymous",
      "dateUpdated": "2022-05-25 11:26:27.998",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eHere is a piece of JSON configuration we’re going to use to update an already existent connector, the one we had configured with the Cypher template strategy on the \u003ccode\u003emovie\u003c/code\u003e topic, and configuring also a DLQ topic.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\n    \u0026quot;topics\u0026quot;: \u0026quot;movie\u0026quot;,\n    \u0026quot;connector.class\u0026quot;: \u0026quot;streams.kafka.connect.sink.Neo4jSinkConnector\u0026quot;,\n    \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.json.JsonConverter\u0026quot;,\n    \u0026quot;key.converter.schemas.enable\u0026quot;: false,\n    \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.json.JsonConverter\u0026quot;,\n    \u0026quot;value.converter.schemas.enable\u0026quot;: false,\n    \u0026quot;kafka.bootstrap.servers\u0026quot;: \u0026quot;broker:9093\u0026quot;,\n    \u0026quot;errors.tolerance\u0026quot;: \u0026quot;all\u0026quot;,\n    \u0026quot;errors.log.enable\u0026quot;: true,\n    \u0026quot;errors.deadletterqueue.topic.name\u0026quot;: \u0026quot;dlq-movie\u0026quot;,\n    \u0026quot;errors.log.include.messages\u0026quot;: true,\n    \u0026quot;errors.deadletterqueue.topic.replication.factor\u0026quot;: 1,\n    \u0026quot;errors.deadletterqueue.context.headers.enable\u0026quot;: true,\n    \u0026quot;neo4j.server.uri\u0026quot;: \u0026quot;bolt://neo4j:7687\u0026quot;,\n    \u0026quot;neo4j.authentication.basic.username\u0026quot;: \u0026quot;neo4j\u0026quot;,\n    \u0026quot;neo4j.authentication.basic.password\u0026quot;: \u0026quot;password\u0026quot;,\n    \u0026quot;neo4j.encryption.enabled\u0026quot;: false,\n    \u0026quot;neo4j.topic.cypher.movie\u0026quot;: \u0026quot;MERGE (m:Movie {title: event.title}) SET m.released\u003devent.released, m.tagline\u003devent.tagline, m.timestamp\u003dtimestamp() RETURN m\u0026quot;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo create the connector instance we just need to submit the above JSON to the Kafka Connect REST endpoint, as follow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecurl -X PUT http://localhost:8083/connectors/Neo4jSinkConnectorCypher/config -H \u0027Content-Type:application/json\u0027 -H \u0027Accept:application/json\u0027 -d @kafka-connect-sink-cypher-dlq.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePlease note that we\u0026rsquo;ve updated an already existing connector (in particular, the one we\u0026rsquo;ve created for the cypher template strategy).\u003c/p\u003e\n\u003cp\u003eNow our topics list should include also our new \u003ccode\u003edlq-movie\u003c/code\u003e topic. We can verify it by running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker exec -it broker kafka-topics --bootstrap-server broker:9092 --list\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1653476715546_1690879644",
      "id": "paragraph_1653476715546_1690879644",
      "dateCreated": "2022-05-25 11:05:15.546",
      "dateStarted": "2022-05-25 11:26:27.998",
      "dateFinished": "2022-05-25 11:26:28.010",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n### Send bad data\n\nAt this point, if we send some bad event such as:\n\n```json\n{name: \"Ian McKellen\" born: 1939}\n```\n\nusing a `kcat` producer as follow:\n\n```bash\nkcat -P -b localhost:9092 -t movie\n```\n\nWe should get a `DataException` and you can verify it by checking Kafka Connect logs with the following:\n\n```bash\ndocker-compose logs -f connect\n```\n\nFurthermore, checking the content of the `dlq-movie` topic:\n\n```bash\nkcat -C -b localhost:9092 -t dlq-movie\n```\n\nwe\u0027ll exactly find the event we have sent!\n\nWith the Kafkacat tool we can do more! We can see also event\u0027s metadata and in particular the headers:\n\n```bash\nkcat -b localhost:9092 -t dlq-movie -C -o-0 -f \u0027\\nKey (%K bytes): %k  \nValue (%S bytes): %s  \nTimestamp: %T  \nPartition: %p  \nOffset: %o  \nHeaders: %h\\n\u0027\n```\n\nAnd the output should be something like this:\n\n```bash\nKey (-1 bytes):   \nValue (33 bytes): {name: \"Ian McKellen\" born: 1939}  \nTimestamp: 1654252331984  \nPartition: 0  \nOffset: 0  \nHeaders: __connect.errors.topic\u003dmovie,__connect.errors.partition\u003d0,__connect.errors.offset\u003d1,__connect.errors.connector.name\u003dNeo4jSinkConnectorCypher,__connect.errors.task.id\u003d0,__connect.errors.stage\u003dVALUE_CONVERTER,__connect.errors.class.name\u003dorg.apache.kafka.connect.json.JsonConverter,__connect.errors.exception.class.name\u003dorg.apache.kafka.connect.errors.DataException,__connect.errors.exception.message\u003dConverting byte[] to Kafka Connect data failed due to serialization error: ,__connect.errors.exception.stacktrace\u003dorg.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error: \n        at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:324)\n        at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$4(WorkerSinkTask.java:516)\n        at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)\n        at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)\n        at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:516)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:493)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:332)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)\n        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)\n        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n        at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.kafka.common.errors.SerializationException: com.fasterxml.jackson.core.JsonParseException: Unexpected character (\u0027n\u0027 (code 110)): was expecting double-quote to start field name\n at [Source: (byte[])\"{name: \"Ian McKellen\" born: 1939}\"; line: 1, column: 3]\n        at org.apache.kafka.connect.json.JsonDeserializer.deserialize(JsonDeserializer.java:66)\n        at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:322)\n        ... 17 more\nCaused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character (\u0027n\u0027 (code 110)): was expecting double-quote to start field name\n at [Source: (byte[])\"{name: \"Ian McKellen\" born: 1939}\"; line: 1, column: 3]\n        at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337)\n        at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:710)\n        at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:635)\n        at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:2054)\n        at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1705)\n        at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1043)\n        at com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:268)\n        at com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:69)\n        at com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:16)\n        at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)\n        at com.fasterxml.jackson.databind.ObjectMapper._readTreeAndClose(ObjectMapper.java:4635)\n        at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:3056)\n        at org.apache.kafka.connect.json.JsonDeserializer.deserialize(JsonDeserializer.java:64)\n        ... 18 more\n```\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-03 11:56:57.629",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSend bad data\u003c/h3\u003e\n\u003cp\u003eAt this point, if we send some bad event such as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-json\"\u003e{name: \u0026quot;Ian McKellen\u0026quot; born: 1939}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eusing a \u003ccode\u003ekcat\u003c/code\u003e producer as follow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-bash\"\u003ekcat -P -b localhost:9092 -t movie\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe should get a \u003ccode\u003eDataException\u003c/code\u003e and you can verify it by checking Kafka Connect logs with the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-bash\"\u003edocker-compose logs -f connect\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFurthermore, checking the content of the \u003ccode\u003edlq-movie\u003c/code\u003e topic:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-bash\"\u003ekcat -C -b localhost:9092 -t dlq-movie\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewe\u0026rsquo;ll exactly find the event we have sent!\u003c/p\u003e\n\u003cp\u003eWith the Kafkacat tool we can do more! We can see also event\u0026rsquo;s metadata and in particular the headers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-bash\"\u003ekcat -b localhost:9092 -t dlq-movie -C -o-0 -f \u0027\\nKey (%K bytes): %k  \nValue (%S bytes): %s  \nTimestamp: %T  \nPartition: %p  \nOffset: %o  \nHeaders: %h\\n\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd the output should be something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-bash\"\u003eKey (-1 bytes):   \nValue (33 bytes): {name: \u0026quot;Ian McKellen\u0026quot; born: 1939}  \nTimestamp: 1654252331984  \nPartition: 0  \nOffset: 0  \nHeaders: __connect.errors.topic\u003dmovie,__connect.errors.partition\u003d0,__connect.errors.offset\u003d1,__connect.errors.connector.name\u003dNeo4jSinkConnectorCypher,__connect.errors.task.id\u003d0,__connect.errors.stage\u003dVALUE_CONVERTER,__connect.errors.class.name\u003dorg.apache.kafka.connect.json.JsonConverter,__connect.errors.exception.class.name\u003dorg.apache.kafka.connect.errors.DataException,__connect.errors.exception.message\u003dConverting byte[] to Kafka Connect data failed due to serialization error: ,__connect.errors.exception.stacktrace\u003dorg.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error: \n        at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:324)\n        at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$4(WorkerSinkTask.java:516)\n        at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)\n        at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)\n        at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:516)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:493)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:332)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)\n        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)\n        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)\n        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:243)\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n        at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.kafka.common.errors.SerializationException: com.fasterxml.jackson.core.JsonParseException: Unexpected character (\u0027n\u0027 (code 110)): was expecting double-quote to start field name\n at [Source: (byte[])\u0026quot;{name: \u0026quot;Ian McKellen\u0026quot; born: 1939}\u0026quot;; line: 1, column: 3]\n        at org.apache.kafka.connect.json.JsonDeserializer.deserialize(JsonDeserializer.java:66)\n        at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:322)\n        ... 17 more\nCaused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character (\u0027n\u0027 (code 110)): was expecting double-quote to start field name\n at [Source: (byte[])\u0026quot;{name: \u0026quot;Ian McKellen\u0026quot; born: 1939}\u0026quot;; line: 1, column: 3]\n        at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337)\n        at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:710)\n        at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:635)\n        at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:2054)\n        at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1705)\n        at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1043)\n        at com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:268)\n        at com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:69)\n        at com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:16)\n        at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)\n        at com.fasterxml.jackson.databind.ObjectMapper._readTreeAndClose(ObjectMapper.java:4635)\n        at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:3056)\n        at org.apache.kafka.connect.json.JsonDeserializer.deserialize(JsonDeserializer.java:64)\n        ... 18 more\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1653476874553_1221846322",
      "id": "paragraph_1653476874553_1221846322",
      "dateCreated": "2022-05-25 11:07:54.553",
      "dateStarted": "2022-06-03 11:56:57.603",
      "dateFinished": "2022-06-03 11:56:58.273",
      "status": "FINISHED"
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-03 11:56:57.572",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654257417570_609210188",
      "id": "paragraph_1654257417570_609210188",
      "dateCreated": "2022-06-03 11:56:57.572",
      "status": "READY"
    }
  ],
  "name": "8. Neo4j Sink Connector - DLQ",
  "id": "2H3BCUTWM",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}